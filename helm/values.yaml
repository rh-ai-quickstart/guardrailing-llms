# Default values for guardrailing-llms.

# Namespace will be automatically set by Helm to {{ .Release.Namespace }}

# Main LLM
mainLLM:
  name: llama-32-3b-instruct
  storageUri: "oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct"
  image: "quay.io/modh/vllm@sha256:db766445a1e3455e1bf7d16b008f8946fcbe9f277377af7abb81ae358805e7e2"
  port: 8080
  
  # Enhanced vLLM features
  enableAutoToolChoice: true
  enableChunkedPrefill: true
  toolCallParser: llama3_json
  chatTemplate: /app/data/template/tool_chat_template_llama3.2_json.jinja
  maxModelLen: 32768
  maxNumSeqs: 8
  
  # Optional GPU tolerations
  tolerations:
    - key: nvidia.com/gpu
      effect: NoSchedule

# Detectors
detectors:
  gibberish:
    storageUri: "oci://quay.io/mmurakam/model-cars:gibberish-text-detector-v0.1.1"
    threshold: 0.35
    port: 8000
  promptInjection:
    storageUri: "oci://quay.io/mmurakam/model-cars:deberta-v3-base-prompt-injection-v2-v0.1.0"
    threshold: 0.5
    port: 8000
  hateAndProfanity:
    storageUri: "oci://quay.io/mmurakam/model-cars:granite-guardian-hap-38m-v0.1.0"
    threshold: 0.5
    port: 8000
  regex:
    port: 8080

# Orchestrator
orchestrator:
  replicas: 1
  port: 8032

# Workbench configuration
workbench:
  enabled: true
  name: guardrails-workbench
  image: image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/jupyter-minimal-cpu-py312-ubi9:2025.1
  resources:
    requests:
      cpu: "1"
      memory: 8Gi
    limits:
      cpu: "2"
      memory: 8Gi
  storage:
    size: 1Gi
  gitRepo:
    url: https://github.com/rh-ai-quickstart/guardrailing-llms.git
    enabled: true

# Cluster configuration  
clusterdomainurl: cluster.example.com
